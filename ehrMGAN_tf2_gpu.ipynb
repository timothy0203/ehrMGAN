{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ehrMGAN: Electronic Health Records Multiple Generative Adversarial Networks\n",
    "This notebook implements the complete ehrMGAN model for generating synthetic electronic health records data. The model combines VAEs and GANs to generate both continuous (vital signs) and discrete (medications/interventions) time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "First, let's set up our environment with the necessary imports and check the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.5.0\n",
      "Memory growth enabled for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# ====== Import necessary libraries ======\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import timeit\n",
    "import argparse\n",
    "import warnings\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import GPUtil\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure TensorFlow 2.x behavior\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "if tf.__version__.startswith('2'):\n",
    "    # Enable memory growth for GPU\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(f\"Memory growth enabled for {device}\")\n",
    "else:\n",
    "    print(\"Warning: This code requires TensorFlow 2.x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.5.0\n",
      "NumPy version: 1.20.3\n",
      "Pandas version: 1.5.2\n",
      "System: win32\n",
      "Python version: 3.8.8\n",
      "Processor: AMD64 Family 25 Model 33 Stepping 0, AuthenticAMD\n",
      "Total memory: 31.93 GB\n",
      "Available memory: 16.46 GB\n",
      "GPU 0: NVIDIA GeForce RTX 3060, Memory: 12288.0 MB\n",
      "   Memory used: 726.0 MB, Load: 3.0%\n",
      "\n",
      "TensorFlow GPU availability:\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# ====== System Information ======\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"System: {sys.platform}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Processor information\n",
    "import platform\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "\n",
    "# Memory information\n",
    "import psutil\n",
    "print(f\"Total memory: {psutil.virtual_memory().total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available memory: {psutil.virtual_memory().available / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "try:\n",
    "    # GPU information\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i}: {gpu.name}, Memory: {gpu.memoryTotal} MB\")\n",
    "        print(f\"   Memory used: {gpu.memoryUsed} MB, Load: {gpu.load*100:.1f}%\")\n",
    "except:\n",
    "    print(\"No GPU detected or GPUtil not installed\")\n",
    "\n",
    "# Check TensorFlow GPU\n",
    "print(\"\\nTensorFlow GPU availability:\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Model Components\n",
    "Now let's import the necessary model components from our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Load modules ======\n",
    "# Import our custom modules\n",
    "from m3gan_tf2 import m3gan\n",
    "from networks_tf2 import C_VAE_NET, D_VAE_NET, C_GAN_NET, D_GAN_NET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Loading and Preprocessing\n",
    "Let's load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous data shape: (16062, 24, 5)\n",
      "Discrete data shape: (16062, 24, 1)\n",
      "Static labels shape: (16062, 1)\n"
     ]
    }
   ],
   "source": [
    "# ====== Load data ======\n",
    "# Dataset parameters\n",
    "patinet_num = 16062\n",
    "filename_postfix = '5_var'\n",
    "\n",
    "# Load continuous data (vital signs)\n",
    "continuous_x = np.loadtxt(f'data/real/mimic/vital_sign_24hrs_{filename_postfix}_mimiciv.txt')\n",
    "continuous_x = continuous_x.reshape(patinet_num, 24, 5)\n",
    "c_dim = continuous_x.shape[-1]\n",
    "\n",
    "# Load discrete data (medications/interventions)\n",
    "discrete_x = np.loadtxt(f'data/real/mimic/med_interv_24hrs_{filename_postfix}_mimiciv.txt')\n",
    "discrete_x = discrete_x.reshape(patinet_num, 24, 1)\n",
    "d_dim = discrete_x.shape[-1]\n",
    "\n",
    "# Load static data (patient demographics)\n",
    "statics_label = pd.read_csv(f'data/real/mimic/static_data_{filename_postfix}_mimiciv.csv')\n",
    "statics_label = np.asarray(statics_label)[:, 0].reshape([-1, 1])\n",
    "\n",
    "print(f\"Continuous data shape: {continuous_x.shape}\")\n",
    "print(f\"Discrete data shape: {discrete_x.shape}\")\n",
    "print(f\"Static labels shape: {statics_label.shape}\")\n",
    "\n",
    "# Data parameters\n",
    "time_steps = continuous_x.shape[1]\n",
    "conditional = True   # Whether to use conditional GAN\n",
    "num_labels = 1 if conditional else 0  # Number of conditional labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Configuration and Hyperparameter Tuning\n",
    "Let's define our hyperparameters based on the data dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters set based on data dimensions:\n",
      "Continuous latent dim: 32, Noise dim: 64\n",
      "Discrete latent dim: 16, Noise dim: 32\n",
      "Encoder size: 128, Decoder size: 128\n"
     ]
    }
   ],
   "source": [
    "# ====== Model hyperparameters ======\n",
    "# Batch size and epochs\n",
    "batch_size = 64\n",
    "num_pre_epochs = 50   # Epochs for pretraining\n",
    "num_epochs = 200      # Epochs for adversarial training\n",
    "\n",
    "# VAE parameters\n",
    "c_z_size = 32         # Latent dimension for continuous VAE\n",
    "d_z_size = 16         # Latent dimension for discrete VAE\n",
    "c_noise_dim = 64      # Noise dimension for continuous generator\n",
    "d_noise_dim = 32      # Noise dimension for discrete generator\n",
    "\n",
    "# Network architecture\n",
    "enc_size = 128        # Hidden units in encoder LSTM\n",
    "dec_size = 128        # Hidden units in decoder LSTM\n",
    "enc_layers = 1        # Number of encoder layers\n",
    "dec_layers = 1        # Number of decoder layers\n",
    "gen_num_units = 128   # Hidden units in generator\n",
    "gen_num_layers = 1    # Number of generator layers\n",
    "dis_num_units = 128   # Hidden units in discriminator\n",
    "dis_num_layers = 1    # Number of discriminator layers\n",
    "keep_prob = 0.8       # Keep probability for dropout\n",
    "l2_scale = 0.001      # L2 regularization scale\n",
    "\n",
    "# Training parameters\n",
    "d_rounds = 1          # Discriminator training rounds per step\n",
    "g_rounds = 1          # Generator training rounds per step\n",
    "v_rounds = 1          # VAE training rounds per step\n",
    "\n",
    "# Learning rates\n",
    "v_lr_pre = 0.001      # VAE pretraining learning rate\n",
    "v_lr = 0.0001         # VAE learning rate\n",
    "g_lr = 0.0001         # Generator learning rate\n",
    "d_lr = 0.0001         # Discriminator learning rate\n",
    "\n",
    "# Weight parameters for losses\n",
    "alpha_re = 1.0        # Reconstruction loss weight\n",
    "alpha_kl = 0.1        # KL divergence loss weight\n",
    "alpha_mt = 2.0        # Matching loss weight\n",
    "alpha_ct = 0.0        # Contrastive loss weight\n",
    "alpha_sm = 0.0        # Smoothness loss weight\n",
    "c_beta_adv = 1.0      # Continuous adversarial loss weight\n",
    "c_beta_fm = 1.0       # Continuous feature matching loss weight\n",
    "d_beta_adv = 1.0      # Discrete adversarial loss weight\n",
    "d_beta_fm = 1.0       # Discrete feature matching loss weight\n",
    "\n",
    "# Adjust hyperparameters based on data dimensions\n",
    "if c_dim > 10:\n",
    "    c_z_size = 64\n",
    "    c_noise_dim = 128\n",
    "    enc_size = 256\n",
    "    dec_size = 256\n",
    "\n",
    "if d_dim > 10:\n",
    "    d_z_size = 32\n",
    "    d_noise_dim = 64\n",
    "\n",
    "print(\"Hyperparameters set based on data dimensions:\")\n",
    "print(f\"Continuous latent dim: {c_z_size}, Noise dim: {c_noise_dim}\")\n",
    "print(f\"Discrete latent dim: {d_z_size}, Noise dim: {d_noise_dim}\")\n",
    "print(f\"Encoder size: {enc_size}, Decoder size: {dec_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build Model Components\n",
    "Now let's build the model components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model components...\n",
      "Continuous VAE created.\n",
      "Discrete VAE created.\n",
      "Continuous GAN created.\n",
      "Discrete GAN created.\n"
     ]
    }
   ],
   "source": [
    "# ====== Create model instances ======\n",
    "# Create VAE instances\n",
    "print(\"Creating model components...\")\n",
    "c_vae = C_VAE_NET(\n",
    "    batch_size=batch_size, time_steps=time_steps, \n",
    "    dim=c_dim, z_dim=c_z_size,\n",
    "    enc_size=enc_size, dec_size=dec_size, \n",
    "    enc_layers=enc_layers, dec_layers=dec_layers, \n",
    "    keep_prob=keep_prob, l2scale=l2_scale,\n",
    "    conditional=conditional, num_labels=num_labels\n",
    ")\n",
    "print(\"Continuous VAE created.\")\n",
    "d_vae = D_VAE_NET(\n",
    "    batch_size=batch_size, time_steps=time_steps, \n",
    "    dim=d_dim, z_dim=d_z_size,\n",
    "    enc_size=enc_size, dec_size=dec_size, \n",
    "    enc_layers=enc_layers, dec_layers=dec_layers, \n",
    "    keep_prob=keep_prob, l2scale=l2_scale,\n",
    "    conditional=conditional, num_labels=num_labels\n",
    ")\n",
    "print(\"Discrete VAE created.\")\n",
    "# Create GAN instances\n",
    "c_gan = C_GAN_NET(\n",
    "    batch_size=batch_size, noise_dim=c_noise_dim, \n",
    "    dim=c_dim, gen_dim=c_z_size, time_steps=time_steps,\n",
    "    gen_num_units=gen_num_units, gen_num_layers=gen_num_layers,\n",
    "    dis_num_units=dis_num_units, dis_num_layers=dis_num_layers,\n",
    "    keep_prob=keep_prob, l2_scale=l2_scale,\n",
    "    conditional=conditional, num_labels=num_labels\n",
    ")\n",
    "print(\"Continuous GAN created.\")\n",
    "d_gan = D_GAN_NET(\n",
    "    batch_size=batch_size, noise_dim=d_noise_dim, \n",
    "    dim=d_dim, gen_dim=d_z_size, time_steps=time_steps,\n",
    "    gen_num_units=gen_num_units, gen_num_layers=gen_num_layers,\n",
    "    dis_num_units=dis_num_units, dis_num_layers=dis_num_layers,\n",
    "    keep_prob=keep_prob, l2_scale=l2_scale,\n",
    "    conditional=conditional, num_labels=num_labels\n",
    ")\n",
    "print(\"Discrete GAN created.\")\n",
    "# Define checkpoint directory\n",
    "checkpoint_dir = \"data/checkpoint/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create and Train the Model\n",
    "Now let's set up the complete model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ====== Create and build the complete model ======\n",
    "# Create the main M3GAN model\n",
    "print(\"Creating the complete M3GAN model...\")\n",
    "model = m3gan(\n",
    "    batch_size=batch_size,\n",
    "    time_steps=time_steps,\n",
    "    num_pre_epochs=num_pre_epochs,\n",
    "    num_epochs=num_epochs,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    epoch_ckpt_freq=100,  # Save checkpoint every 100 epochs\n",
    "    epoch_loss_freq=10,   # Display loss every 10 epochs\n",
    "    \n",
    "    # Continuous parameters\n",
    "    c_dim=c_dim,\n",
    "    c_noise_dim=c_noise_dim,\n",
    "    c_z_size=c_z_size,\n",
    "    c_data_sample=continuous_x,\n",
    "    c_vae=c_vae,\n",
    "    c_gan=c_gan,\n",
    "    \n",
    "    # Discrete parameters\n",
    "    d_dim=d_dim,\n",
    "    d_noise_dim=d_noise_dim,\n",
    "    d_z_size=d_z_size,\n",
    "    d_data_sample=discrete_x,\n",
    "    d_vae=d_vae,\n",
    "    d_gan=d_gan,\n",
    "    \n",
    "    # Training parameters\n",
    "    d_rounds=d_rounds,\n",
    "    g_rounds=g_rounds,\n",
    "    v_rounds=v_rounds,\n",
    "    v_lr_pre=v_lr_pre,\n",
    "    v_lr=v_lr,\n",
    "    g_lr=g_lr,\n",
    "    d_lr=d_lr,\n",
    "    \n",
    "    # Loss weights\n",
    "    alpha_re=alpha_re,\n",
    "    alpha_kl=alpha_kl,\n",
    "    alpha_mt=alpha_mt,\n",
    "    alpha_ct=alpha_ct,\n",
    "    alpha_sm=alpha_sm,\n",
    "    c_beta_adv=c_beta_adv,\n",
    "    c_beta_fm=c_beta_fm,\n",
    "    d_beta_adv=d_beta_adv,\n",
    "    d_beta_fm=d_beta_fm,\n",
    "    \n",
    "    # Conditional parameters\n",
    "    conditional=conditional,\n",
    "    num_labels=num_labels,\n",
    "    statics_label=statics_label\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Train the Model ======\n",
    "# Train the model using the TF2.x-compatible train method\n",
    "print(\"Starting training...\")\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Check if we want to train the model\n",
    "train_model = True\n",
    "if train_model:\n",
    "    model.train()\n",
    "    print(\"Training completed!\")\n",
    "else:\n",
    "    print(\"Skipping training phase.\")\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(f\"Training time: {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Addressing First Hour Low Variance Issue\n",
    "Let's analyze and fix the issue with low standard deviation in the first hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data first\n",
    "print(\"Generating synthetic data...\")\n",
    "d_gen_data, c_gen_data = model.generate_data(num_sample=1024)\n",
    "\n",
    "# Apply renormalization if your data was normalized\n",
    "print(\"Renormalizing generated data...\")\n",
    "c_gen_data_renorm = c_gen_data  # If no renormalization needed\n",
    "# If renormalization needed:\n",
    "# c_gen_data_renorm = renormlizer(c_gen_data, data_info) # TODO: Implement renormlizer\n",
    "\n",
    "\n",
    "\n",
    "# Analyze the first hour variance issue\n",
    "print(\"Analyzing variance across time steps...\")\n",
    "real_std_by_hour = np.std(continuous_x, axis=0)\n",
    "gen_std_by_hour = np.std(c_gen_data, axis=0)\n",
    "\n",
    "# Plot the standard deviation over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(c_dim):\n",
    "    plt.subplot(1, c_dim, i+1)\n",
    "    plt.plot(range(time_steps), real_std_by_hour[:, i], 'b-', label='Real')\n",
    "    plt.plot(range(time_steps), gen_std_by_hour[:, i], 'r-', label='Generated')\n",
    "    plt.title(f'Feature {i+1} Std Dev')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fix the low variance issue with a post-processing step\n",
    "print(\"\\nApplying variance correction to the first hour...\")\n",
    "\n",
    "# Improved variance matching function with consistency preservation\n",
    "def variance_matching(data, target_std, axis=0, smoothing_factor=0.7):\n",
    "    \"\"\"\n",
    "    Match variance of data to target_std while preserving mean and temporal consistency\n",
    "    \n",
    "    Args:\n",
    "        data: The data to adjust\n",
    "        target_std: The target standard deviation\n",
    "        axis: Axis along which to compute statistics\n",
    "        smoothing_factor: How much to smooth between original and adjusted values (0-1)\n",
    "                         Higher means more of the original preserved\n",
    "    \"\"\"\n",
    "    mean = np.mean(data, axis=axis, keepdims=True)\n",
    "    std = np.std(data, axis=axis, keepdims=True) + 1e-10  # Avoid division by zero\n",
    "    \n",
    "    # Normalize the data\n",
    "    normalized = (data - mean) / std\n",
    "    \n",
    "    # Scale to target std dev and shift back to original mean\n",
    "    adjusted = normalized * target_std + mean\n",
    "    \n",
    "    # Apply smoothing to preserve some of the original structure\n",
    "    result = smoothing_factor * data + (1 - smoothing_factor) * adjusted\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply the correction to the first hour with improvements\n",
    "print(\"\\nApplying enhanced variance correction to the first hour...\")\n",
    "c_gen_data_corrected = c_gen_data_renorm.copy()\n",
    "\n",
    "for i in range(c_dim):\n",
    "    # First hour data\n",
    "    first_hour_data = c_gen_data_corrected[:, 0, i].reshape(-1, 1)\n",
    "    \n",
    "    # Target std dev with slight randomization for more natural results\n",
    "    target_std = real_std_by_hour[0, i] * np.random.uniform(1.0, 1.15)\n",
    "    \n",
    "    # Apply variance matching with trajectory consistency preservation\n",
    "    c_gen_data_corrected[:, 0, i] = variance_matching(\n",
    "        first_hour_data, target_std, axis=0, smoothing_factor=0.3\n",
    "    ).flatten()\n",
    "    \n",
    "    # For natural transitions, also slightly adjust the second hour\n",
    "    if time_steps > 1:\n",
    "        second_hour_data = c_gen_data_corrected[:, 1, i].reshape(-1, 1)\n",
    "        target_std_2 = real_std_by_hour[1, i] * np.random.uniform(0.95, 1.05)\n",
    "        c_gen_data_corrected[:, 1, i] = variance_matching(\n",
    "            second_hour_data, target_std_2, axis=0, smoothing_factor=0.6\n",
    "        ).flatten()\n",
    "\n",
    "# Verify the correction\n",
    "corrected_std_by_hour = np.std(c_gen_data_corrected, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(c_dim):\n",
    "    plt.subplot(1, c_dim, i+1)\n",
    "    plt.plot(range(time_steps), real_std_by_hour[:, i], 'b-', label='Real')\n",
    "    plt.plot(range(time_steps), gen_std_by_hour[:, i], 'r--', label='Original Generated')\n",
    "    plt.plot(range(time_steps), corrected_std_by_hour[:, i], 'g-', label='Corrected')\n",
    "    plt.title(f'Feature {i+1} Std Dev')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the corrected data\n",
    "save_path_corrected = f\"data/fake/gen_data_mimiciv_{filename_postfix}_corrected.npz\"\n",
    "np.savez(save_path_corrected, \n",
    "         c_gen_data=c_gen_data_corrected, \n",
    "         d_gen_data=d_gen_data)\n",
    "print(f\"Corrected generated data saved to {save_path_corrected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and visualize a few trajectories\n",
    "num_samples_to_visualize = 5\n",
    "sample_indices = np.random.choice(c_gen_data.shape[0], num_samples_to_visualize, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Plot continuous features\n",
    "    for j in range(c_dim):\n",
    "        plt.subplot(num_samples_to_visualize, c_dim, i*c_dim + j + 1)\n",
    "        plt.plot(range(time_steps), continuous_x[idx, :, j], 'b-', label='Real')\n",
    "        plt.plot(range(time_steps), c_gen_data_renorm[idx, :, j], 'r-', label='Generated')\n",
    "        if i == 0 and j == 0:\n",
    "            plt.legend()\n",
    "        plt.title(f'Sample {i+1}, Feature {j+1}')\n",
    "        plt.xlabel('Hour')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For discrete data\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    plt.subplot(1, num_samples_to_visualize, i + 1)\n",
    "    plt.step(range(time_steps), discrete_x[idx, :, 0], 'b-', where='post', label='Real')\n",
    "    plt.step(range(time_steps), d_gen_data[idx, :, 0], 'r-', where='post', label='Generated')\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "    plt.title(f'Discrete Sample {i+1}')\n",
    "    plt.xlabel('Hour')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare distributions\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(c_dim):\n",
    "    plt.subplot(1, c_dim, i + 1)\n",
    "    sns.kdeplot(continuous_x[:, :, i].flatten(), label='Real', color='blue')\n",
    "    sns.kdeplot(c_gen_data_renorm[:, :, i].flatten(), label='Generated', color='red')\n",
    "    plt.title(f'Feature {i+1} Distribution')\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Discrete data distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "bins = np.linspace(0, 1, 20)\n",
    "plt.hist(discrete_x.flatten(), bins=bins, alpha=0.5, label='Real', density=True)\n",
    "plt.hist(d_gen_data.flatten(), bins=bins, alpha=0.5, label='Generated', density=True)\n",
    "plt.legend()\n",
    "plt.title('Discrete Data Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Advanced Analysis: Preserving Medical Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations between features\n",
    "def plot_correlation_heatmap(data, title):\n",
    "    # Reshape to (patients*times, features)\n",
    "    data_flat = data.reshape(-1, data.shape[2])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr = np.corrcoef(data_flat.T)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return corr\n",
    "\n",
    "# Plot correlation heatmaps\n",
    "real_corr = plot_correlation_heatmap(continuous_x, 'Real Data Correlation')\n",
    "gen_corr = plot_correlation_heatmap(c_gen_data_renorm, 'Generated Data Correlation')\n",
    "\n",
    "# Calculate correlation difference\n",
    "corr_diff = np.abs(real_corr - gen_corr)\n",
    "print(f\"Mean absolute correlation difference: {np.mean(corr_diff):.4f}\")\n",
    "\n",
    "# Visualize temporal patterns\n",
    "def plot_mean_trajectory(data, title):\n",
    "    mean_trajectory = np.mean(data, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(data.shape[2]):\n",
    "        plt.subplot(1, data.shape[2], i+1)\n",
    "        plt.plot(range(data.shape[1]), mean_trajectory[:, i])\n",
    "        plt.title(f'Feature {i+1}')\n",
    "        plt.xlabel('Hour')\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "plot_mean_trajectory(continuous_x, 'Real Data Mean Trajectory')\n",
    "plot_mean_trajectory(c_gen_data_renorm, 'Generated Data Mean Trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model to distinguish real from generated data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def train_discriminator(real, gen):\n",
    "    # Prepare data\n",
    "    real_flat = real.reshape(real.shape[0], -1)\n",
    "    gen_flat = gen.reshape(gen.shape[0], -1)\n",
    "    \n",
    "    # Combine and create labels\n",
    "    X = np.vstack([real_flat, gen_flat])\n",
    "    y = np.concatenate([np.zeros(len(real_flat)), np.ones(len(gen_flat))])\n",
    "    \n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create a simple discriminator model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])  # Use 'acc' for TF1.x\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1,\n",
    "        batch_size=128\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    apr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    return auc, apr, history\n",
    "\n",
    "# Evaluate discriminative performance\n",
    "print(\"Training discriminator for continuous data...\")\n",
    "c_auc, c_apr, c_history = train_discriminator(continuous_x, c_gen_data_renorm)\n",
    "print(f\"Continuous data - AUC: {c_auc:.4f}, APR: {c_apr:.4f}\")\n",
    "\n",
    "print(\"\\nTraining discriminator for discrete data...\")\n",
    "d_auc, d_apr, d_history = train_discriminator(discrete_x, d_gen_data)\n",
    "print(f\"Discrete data - AUC: {d_auc:.4f}, APR: {d_apr:.4f}\")\n",
    "\n",
    "# In an ideal GAN, AUC should be close to 0.5 (indistinguishable)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(c_history.history['acc'], label='train')\n",
    "plt.plot(c_history.history['val_acc'], label='test')\n",
    "plt.title('Continuous Discriminator Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(d_history.history['acc'], label='train')\n",
    "plt.plot(d_history.history['val_acc'], label='test')\n",
    "plt.title('Discrete Discriminator Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "print(\"M3GAN Model Training Summary\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Dataset: MIMIC-IV with {patinet_num} patients\")\n",
    "print(f\"Features: {c_dim} continuous, {d_dim} discrete\")\n",
    "print(f\"Time steps: {time_steps} hours\")\n",
    "print(f\"Pre-training epochs: {num_pre_epochs}\")\n",
    "print(f\"Training epochs: {num_epochs}\")\n",
    "print(\"\\nGenerated data quality:\")\n",
    "print(f\"Continuous data AUC: {c_auc:.4f} (closer to 0.5 is better)\")\n",
    "print(f\"Discrete data AUC: {d_auc:.4f} (closer to 0.5 is better)\")\n",
    "print(f\"Mean correlation difference: {np.mean(corr_diff):.4f} (lower is better)\")\n",
    "print(\"\\nTraining files:\")\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "print(f\"Generated data: data/fake/gen_data_mimiciv_{filename_postfix}_corrected.npz\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. For low variance in first hour: Applied variance matching as a post-processing step\")\n",
    "print(\"2. For better stability: Consider using gradient penalty or spectral normalization\")\n",
    "print(\"3. For better feature correlations: Increase the alpha_mt parameter\")\n",
    "print(\"4. For more realistic trajectories: Increase the number of GAN training epochs\")\n",
    "print(\"5. For more diverse samples: Consider reducing batch size or adding noise during generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Close Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the TensorFlow session\n",
    "sess.close()\n",
    "print(\"TensorFlow session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
